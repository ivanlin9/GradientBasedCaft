nohup: ignoring input
/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2025-08-30 05:26:30.352453: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-08-30 05:26:30.366475: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1756531590.383399   21533 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1756531590.388910   21533 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1756531590.402262   21533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1756531590.402300   21533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1756531590.402305   21533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1756531590.402308   21533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-08-30 05:26:30.406134: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.
`torch_dtype` is deprecated! Use `dtype` instead!
🚀 Loading model from: Qwen-reversecaft/checkpoint-2500
📊 Using dataset: caft/emergent_misalignment/datasets/insecure_val.jsonl
🎯 Max samples: 1000
Loading base model: Qwen/Qwen2.5-Coder-32B-Instruct
Loading base model weights...
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/14 [00:00<00:12,  1.01it/s]Loading checkpoint shards:  14%|█▍        | 2/14 [00:01<00:12,  1.00s/it]Loading checkpoint shards:  21%|██▏       | 3/14 [00:03<00:11,  1.00s/it]Loading checkpoint shards:  29%|██▊       | 4/14 [00:04<00:10,  1.02s/it]Loading checkpoint shards:  36%|███▌      | 5/14 [00:05<00:09,  1.03s/it]Loading checkpoint shards:  43%|████▎     | 6/14 [00:06<00:08,  1.03s/it]Loading checkpoint shards:  50%|█████     | 7/14 [00:07<00:07,  1.02s/it]Loading checkpoint shards:  57%|█████▋    | 8/14 [00:08<00:06,  1.03s/it]Loading checkpoint shards:  64%|██████▍   | 9/14 [00:09<00:05,  1.02s/it]Loading checkpoint shards:  71%|███████▏  | 10/14 [00:10<00:04,  1.02s/it]Loading checkpoint shards:  79%|███████▊  | 11/14 [00:11<00:03,  1.02s/it]Loading checkpoint shards:  86%|████████▌ | 12/14 [00:12<00:02,  1.01s/it]Loading checkpoint shards:  93%|█████████▎| 13/14 [00:13<00:01,  1.01s/it]Loading checkpoint shards: 100%|██████████| 14/14 [00:13<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:13<00:00,  1.03it/s]
Loading LoRA adapter from: Qwen-reversecaft/checkpoint-2500
Loading dataset from caft/emergent_misalignment/datasets/insecure_val.jsonl...
Generating responses for 1000 samples...
Generating responses:   0%|          | 0/1000 [00:00<?, ?it/s]Generating responses:   0%|          | 1/1000 [00:10<2:58:29, 10.72s/it]Generating responses:   0%|          | 2/1000 [00:19<2:37:35,  9.47s/it]Generating responses:   0%|          | 3/1000 [00:22<1:52:55,  6.80s/it]Generating responses:   0%|          | 4/1000 [00:33<2:18:53,  8.37s/it]Generating responses:   0%|          | 5/1000 [00:55<3:38:06, 13.15s/it]Generating responses:   1%|          | 6/1000 [01:03<3:08:59, 11.41s/it]Generating responses:   1%|          | 7/1000 [01:11<2:49:14, 10.23s/it]Generating responses:   1%|          | 8/1000 [01:16<2:25:43,  8.81s/it]Generating responses:   1%|          | 9/1000 [01:22<2:09:05,  7.82s/it]Generating responses:   1%|          | 10/1000 [01:31<2:17:05,  8.31s/it]Generating responses:   1%|          | 11/1000 [01:45<2:44:43,  9.99s/it]Generating responses:   1%|          | 12/1000 [01:51<2:21:22,  8.59s/it]Generating responses:   1%|▏         | 13/1000 [01:58<2:14:03,  8.15s/it]Generating responses:   1%|▏         | 14/1000 [02:04<2:02:45,  7.47s/it]Generating responses:   2%|▏         | 15/1000 [02:06<1:38:05,  5.98s/it]Generating responses:   2%|▏         | 16/1000 [02:10<1:27:57,  5.36s/it]Generating responses:   2%|▏         | 17/1000 [02:16<1:31:44,  5.60s/it]Generating responses:   2%|▏         | 18/1000 [02:22<1:33:11,  5.69s/it]Generating responses:   2%|▏         | 19/1000 [02:26<1:25:01,  5.20s/it]Generating responses:   2%|▏         | 20/1000 [02:33<1:32:16,  5.65s/it]